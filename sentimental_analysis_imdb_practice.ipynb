{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/haoli/Downloads/imdb/train.csv')\n",
    "test = pd.read_csv('/Users/haoli/Downloads/imdb/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  For a movie that gets no respect there sure ar...       pos\n",
       "1  Bizarre horror movie filled with famous faces ...       pos\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...       pos\n",
       "3  It's a strange feeling to sit alone in a theat...       pos\n",
       "4  You probably all already know this by now, but...       pos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on an actual story, John Boorman shows t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a gem. As a Film Four production - the...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like this show. It has drama, romance...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the best 3-D experience Disney has at ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of the Korean movies I've seen, only three had...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Based on an actual story, John Boorman shows t...       pos\n",
       "1  This is a gem. As a Film Four production - the...       pos\n",
       "2  I really like this show. It has drama, romance...       pos\n",
       "3  This is the best 3-D experience Disney has at ...       pos\n",
       "4  Of the Korean movies I've seen, only three had...       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    12500\n",
       "neg    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    12500\n",
       "neg    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.The naive Bayesian model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to transform a sentence (or an article) into a vector first. A naive way of doing this is the following:\n",
    "#Count all the different words showing up in all of the texts, call this collection vocabulary. Then each article\n",
    "#is transform into a vector of length the length of the vocabulary by the rule: If it has a certain word, that \n",
    "#position is the number of times that word appears in the article, otherwise it is 0. In this way we get a very \n",
    "#sparse vector for each article. Of course the order of the words is lost in this kind of encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, tokenization: Removing the punctuation symbols and other weird things from the article, and break the \n",
    "#resulting article into a vector of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to replace the punctuations by ' '.\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the above string into a string consists of ' ' of equal length.\n",
    "transtbl = str.maketrans(string.punctuation, ' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words show up frequently, we don't want to take them into account.\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(s):\n",
    "#The symbol '<br />' also appears in the articles, first remove them.\n",
    "    s = s.replace('<br />', '')\n",
    "    \n",
    "#Next, replace punctuations by ' '\n",
    "    s = s.translate(transtbl)\n",
    "    \n",
    "#lemmatize(..., 'v') changes the verb like 'loved' into 'love'.\n",
    "#The stopwords are removed.\n",
    "    tokens = [lemmatizer.lemmatize(t.lower(),'v') \n",
    "              for t in nltk.word_tokenize(s) \n",
    "              if t.lower() not in stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So the above function returns a string separated by ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoli/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/haoli/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#import a progress bar to show the progress.\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145bd65491ad44f2a4979b397a25cf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb04078204848cdafb168abfa8a230a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for df in train, test:\n",
    "    df['text_prep'] = df['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e510c24a1794a67a57ca11b56a6e404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#All the words appeared in the training articles with repetitions.\n",
    "all_words = [word for text in tqdm_notebook(train['text_prep']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'get',\n",
       " 'respect',\n",
       " 'sure',\n",
       " 'lot',\n",
       " 'memorable',\n",
       " 'quote',\n",
       " 'list',\n",
       " 'gem',\n",
       " 'imagine']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 65081 samples and 3022531 outcomes>\n"
     ]
    }
   ],
   "source": [
    "#FreqDist counts the frequency of each word in \n",
    "voca = nltk.FreqDist(all_words)\n",
    "print(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 48170, 'movie': 43912, 'one': 26747, 'make': 23538, 'like': 22335, 'see': 20773, 'get': 18108, 'time': 16143, 'good': 15124, 'character': 14153, ...})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 48170),\n",
       " ('movie', 43912),\n",
       " ('one', 26747),\n",
       " ('make', 23538),\n",
       " ('like', 22335),\n",
       " ('see', 20773),\n",
       " ('get', 18108),\n",
       " ('time', 16143),\n",
       " ('good', 15124),\n",
       " ('character', 14153)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we use all the words appeared, the vectors are going to be too long and sparse. Here use the top 3000 words as\n",
    "#the vocabulary.\n",
    "topwords = [word for word in voca.most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film', 'movie', 'one', 'make', 'like']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer() #The default value of smooth_idf is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use topwords to construct the vectorizer. Otherwise the vectorizer would use the context we are going fit it.\n",
    "tfidf_vec = TfidfVectorizer(vocabulary = topwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train['text_prep'], train['sentiment']\n",
    "test_x, test_y = test['text_prep'], test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tfidf_vec.fit_transform(train_x)\n",
    "test_features = tfidf_vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the multinomial naive Bayesian model.\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We fit the MNB model on the training set. Notice that we don't have to encode the target. \n",
    "mnb.fit(train_features, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on the test set.\n",
    "pred = mnb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.85      0.84     12500\n",
      "         pos       0.85      0.83      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.The DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section we do a practice using dnn model on the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The simple dnn model\n",
    "def build_nn_model(input_dim, layers, output_dim):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "        H = Dropout(0.5)(H)\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unlike MNB, for dnn model we need to have a numerical target. We have to (onehot) encode 'sentiment'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_num = train_y.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_num = test_y.apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to turn 1 and 0 to [1,0] and [0,1].\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_oh = convert_to_one_hot(np.array(train_y_num), 2)\n",
    "test_y_oh = convert_to_one_hot(np.array(test_y_num), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.toarray()\n",
    "test_features = test_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we build up the simple dnn model.\n",
    "dnn_model = build_nn_model(input_dim = 3000, layers = [1000, 300, 100, 30, 10], output_dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 13s 537us/sample - loss: 0.4964 - accuracy: 0.7442 - val_loss: 0.3102 - val_accuracy: 0.8665\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 13s 528us/sample - loss: 0.3434 - accuracy: 0.8742 - val_loss: 0.3196 - val_accuracy: 0.8684\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 13s 525us/sample - loss: 0.2704 - accuracy: 0.9038 - val_loss: 0.3564 - val_accuracy: 0.8662\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 14s 540us/sample - loss: 0.2094 - accuracy: 0.9288 - val_loss: 0.3736 - val_accuracy: 0.8630\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 14s 548us/sample - loss: 0.1315 - accuracy: 0.9594 - val_loss: 0.5613 - val_accuracy: 0.8644\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 14s 553us/sample - loss: 0.0821 - accuracy: 0.9749 - val_loss: 0.8219 - val_accuracy: 0.8643\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 14s 546us/sample - loss: 0.0563 - accuracy: 0.9839 - val_loss: 1.0760 - val_accuracy: 0.8559\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 14s 545us/sample - loss: 0.0469 - accuracy: 0.9837 - val_loss: 1.2471 - val_accuracy: 0.8594\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 13s 538us/sample - loss: 0.0431 - accuracy: 0.9862 - val_loss: 1.2200 - val_accuracy: 0.8593\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 13s 539us/sample - loss: 0.0388 - accuracy: 0.9874 - val_loss: 1.4031 - val_accuracy: 0.8589\n"
     ]
    }
   ],
   "source": [
    "imbd_history = dnn_model.fit(\n",
    "    train_features, \n",
    "    train_y_oh, \n",
    "    epochs=10, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_features, test_y_oh), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(his, metrics):\n",
    "    for metric in metrics:\n",
    "        plt.plot(his.history[metric], label=metric)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(imbd_history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It starts overlfitting the training set after the first epoch. And the largest validation accuracy appears at\n",
    "#the second epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.The MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is similar to the dnn model as above. The difference is that we add dropout layers. \n",
    "def build_mlp_model(input_dim, layers, output_dim, dropout_rate=0.2):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "        H = Dropout(rate=dropout_rate)(H) #Adding a dropout layer after each dense layer.\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = build_mlp_model(input_dim = 3000,\n",
    "                           layers = [600, 120, 24],\n",
    "                           output_dim = 2,\n",
    "                           dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 3000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 600)               1800600   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 120)               72120     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                2904      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,875,674\n",
      "Trainable params: 1,875,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s 274us/sample - loss: 0.3823 - accuracy: 0.8348 - val_loss: 0.2973 - val_accuracy: 0.8744\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 7s 267us/sample - loss: 0.2618 - accuracy: 0.8984 - val_loss: 0.3130 - val_accuracy: 0.8688\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 7s 269us/sample - loss: 0.2054 - accuracy: 0.9250 - val_loss: 0.3507 - val_accuracy: 0.8665\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 6s 256us/sample - loss: 0.1188 - accuracy: 0.9607 - val_loss: 0.4334 - val_accuracy: 0.8620\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 6s 259us/sample - loss: 0.0627 - accuracy: 0.9798 - val_loss: 0.5610 - val_accuracy: 0.8614\n"
     ]
    }
   ],
   "source": [
    "imbd_history = mlp_model.fit(\n",
    "    train_features, \n",
    "    train_y_oh, \n",
    "    epochs=5, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_features, test_y_oh), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It helps a little bit, but the overfitting problem is still there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hyperparameters to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable hyperparameters.\n",
    "hyper_params = {\n",
    "    'learning_rate': 1e-3,  # default for Adam\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 64,\n",
    "    'layers': [64, 32],\n",
    "    'dim': 20000,\n",
    "    'dropout_rate': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                1280064   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282,210\n",
      "Trainable params: 1,282,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = build_mlp_model(\n",
    "    input_dim=hyper_params['dim'],\n",
    "    layers=hyper_params['layers'],\n",
    "    output_dim=2,\n",
    "    dropout_rate=hyper_params['dropout_rate'],\n",
    ")\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(k = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use tfidfvectorizer with default vocabulary, i.e. the words in the articles it fitted on.\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=2, # ignore word that only appears in 1 document\n",
    "    ngram_range=(1, 2), # consider both uni-gram and bi-gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_new = tfidf_vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 351389)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_new = tfidf_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20000, score_func=<function f_classif at 0x11ec297a0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(train_x_new, train_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_20000 = selector.transform(train_x_new)\n",
    "test_x_20000 = selector.transform(test_x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20000)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_20000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20000)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_20000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1845253 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoppping_hook = EarlyStopping(\n",
    "    monitor='val_loss',  # what metrics to track\n",
    "    patience=2,  # maximum number of epochs allowed without imporvement on monitored metrics \n",
    ")\n",
    "\n",
    "CPK_PATH = 'model_cpk.hdf5'    # path to store checkpoint\n",
    "\n",
    "model_cpk_hook = ModelCheckpoint(\n",
    "    CPK_PATH,\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,  # Only keep the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=hyper_params['learning_rate']), \n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s 298us/sample - loss: 0.3502 - accuracy: 0.8562 - val_loss: 0.2632 - val_accuracy: 0.8922\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 4s 175us/sample - loss: 0.1443 - accuracy: 0.9508 - val_loss: 0.3053 - val_accuracy: 0.8809\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 4s 149us/sample - loss: 0.0886 - accuracy: 0.9709 - val_loss: 0.3691 - val_accuracy: 0.8757\n"
     ]
    }
   ],
   "source": [
    "mlp_history = mlp_model.fit(train_x_20000.toarray(), \n",
    "                            train_y_oh, epochs = 5, \n",
    "                            validation_data = [test_x_20000.toarray(), test_y_oh],\n",
    "                            batch_size = hyper_params['batch_size'],\n",
    "                            callbacks=[early_stoppping_hook, model_cpk_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing with stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above we vectorize the text part of the datasets without stopwords. Here we use the stopwords.\n",
    "train_x_stp = tfidf_vectorizer.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_stp = tfidf_vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=20000, score_func=<function f_classif at 0x11ec297a0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(train_x_stp, train_y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_stp = selector.transform(train_x_stp)\n",
    "test_x_stp = selector.transform(test_x_stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20000)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_stp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20000)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_stp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = build_mlp_model(\n",
    "    input_dim=hyper_params['dim'],\n",
    "    layers=hyper_params['layers'],\n",
    "    output_dim=2,\n",
    "    dropout_rate=hyper_params['dropout_rate'],\n",
    ")\n",
    "\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=hyper_params['learning_rate']), \n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 8s 301us/sample - loss: 0.3335 - accuracy: 0.8664 - val_loss: 0.2448 - val_accuracy: 0.9003\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 4s 161us/sample - loss: 0.1327 - accuracy: 0.9539 - val_loss: 0.2565 - val_accuracy: 0.9017\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 4s 148us/sample - loss: 0.0733 - accuracy: 0.9752 - val_loss: 0.3247 - val_accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "mlp_history = mlp_model.fit(train_x_stp.toarray(), \n",
    "                            train_y_oh, epochs = 5, \n",
    "                            validation_data = [test_x_stp.toarray(), test_y_oh],\n",
    "                            batch_size = hyper_params['batch_size'],\n",
    "                            callbacks=[early_stoppping_hook, model_cpk_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that for the same model, using stopwords we get a slightly better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
